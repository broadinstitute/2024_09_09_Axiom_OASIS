{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics and combine across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "output_dir = \"../1_snakemake/outputs\"\n",
    "results_dir = \"./compiled_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_actual, y_prob):\n",
    "    f1 = f1_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_actual, y_prob)\n",
    "    except ValueError:\n",
    "        auroc = None\n",
    "\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_actual, y_prob)\n",
    "        prauc = auc(recall, precision)\n",
    "    except ValueError:\n",
    "        prauc = None\n",
    "    \n",
    "    return f1, auroc, prauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seal outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27560/1642003981.py:11: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  result = grouped.with_columns([\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# CPCNN\n",
    "pred = pl.read_parquet(f\"{output_dir}/cpcnn/mad_featselect/classifier_results/seal_binary_predictions.parquet\")\n",
    "class_balance = pred.select([\"Metadata_AggType\", \"Metadata_Label\", \"Metadata_Count_0\", \"Metadata_Count_1\"]).unique()\n",
    "\n",
    "grouped = pred.group_by(['Metadata_AggType', 'Metadata_Label']).agg([\n",
    "    pl.col('y_pred').alias('y_pred_list'),\n",
    "    pl.col('y_actual').alias('y_actual_list'),\n",
    "    pl.col('y_prob').alias('y_prob_list'),\n",
    "])\n",
    "\n",
    "result = grouped.with_columns([\n",
    "    pl.struct(['y_pred_list', 'y_actual_list', 'y_prob_list']).map_elements(\n",
    "        lambda s: compute_metrics(s['y_pred_list'], s['y_actual_list'], s['y_prob_list'])\n",
    "    ).alias('metrics')\n",
    "])\n",
    "\n",
    "result = result.with_columns([\n",
    "    pl.col('metrics').map_elements(lambda s: s[0]).alias('F1_Score'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('AUROC'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[2]).alias('PRAUC'),\n",
    "])\n",
    "\n",
    "result = result.drop(['y_pred_list', 'y_actual_list', 'y_prob_list', 'metrics'])\n",
    "cpcnn = result.join(class_balance, on=['Metadata_AggType', 'Metadata_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27560/3048713840.py:11: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  result = grouped.with_columns([\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# Dino\n",
    "pred = pl.read_parquet(f\"{output_dir}/dino/mad_featselect/classifier_results/seal_binary_predictions.parquet\")\n",
    "class_balance = pred.select([\"Metadata_AggType\", \"Metadata_Label\", \"Metadata_Count_0\", \"Metadata_Count_1\"]).unique()\n",
    "\n",
    "grouped = pred.group_by(['Metadata_AggType', 'Metadata_Label']).agg([\n",
    "    pl.col('y_pred').alias('y_pred_list'),\n",
    "    pl.col('y_actual').alias('y_actual_list'),\n",
    "    pl.col('y_prob').alias('y_prob_list'),\n",
    "])\n",
    "\n",
    "result = grouped.with_columns([\n",
    "    pl.struct(['y_pred_list', 'y_actual_list', 'y_prob_list']).map_elements(\n",
    "        lambda s: compute_metrics(s['y_pred_list'], s['y_actual_list'], s['y_prob_list'])\n",
    "    ).alias('metrics')\n",
    "])\n",
    "\n",
    "result = result.with_columns([\n",
    "    pl.col('metrics').map_elements(lambda s: s[0]).alias('F1_Score'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('AUROC'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('PRAUC'),\n",
    "])\n",
    "\n",
    "result = result.drop(['y_pred_list', 'y_actual_list', 'y_prob_list', 'metrics'])\n",
    "dino = result.join(class_balance, on=['Metadata_AggType', 'Metadata_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27560/2895542224.py:11: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  result = grouped.with_columns([\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# CellProfiler\n",
    "pred = pl.read_parquet(f\"{output_dir}/cellprofiler/mad_featselect/classifier_results/seal_binary_predictions.parquet\")\n",
    "class_balance = pred.select([\"Metadata_AggType\", \"Metadata_Label\", \"Metadata_Count_0\", \"Metadata_Count_1\"]).unique()\n",
    "\n",
    "grouped = pred.group_by(['Metadata_AggType', 'Metadata_Label']).agg([\n",
    "    pl.col('y_pred').alias('y_pred_list'),\n",
    "    pl.col('y_actual').alias('y_actual_list'),\n",
    "    pl.col('y_prob').alias('y_prob_list'),\n",
    "])\n",
    "\n",
    "result = grouped.with_columns([\n",
    "    pl.struct(['y_pred_list', 'y_actual_list', 'y_prob_list']).map_elements(\n",
    "        lambda s: compute_metrics(s['y_pred_list'], s['y_actual_list'], s['y_prob_list'])\n",
    "    ).alias('metrics')\n",
    "])\n",
    "\n",
    "result = result.with_columns([\n",
    "    pl.col('metrics').map_elements(lambda s: s[0]).alias('F1_Score'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('AUROC'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('PRAUC'),\n",
    "])\n",
    "\n",
    "result = result.drop(['y_pred_list', 'y_actual_list', 'y_prob_list', 'metrics'])\n",
    "cellprofiler = result.join(class_balance, on=['Metadata_AggType', 'Metadata_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "cpcnn = cpcnn.with_columns(pl.lit(\"cpcnn\").alias(\"Feat_type\"))\n",
    "dino = dino.with_columns(pl.lit(\"dino\").alias(\"Feat_type\"))\n",
    "cellprofiler = cellprofiler.with_columns(pl.lit(\"cellprofiler\").alias(\"Feat_type\"))\n",
    "\n",
    "all_results = pl.concat([cpcnn, dino, cellprofiler], how=\"vertical\")\n",
    "all_results.write_parquet(f\"{results_dir}/compiled_seal_metrics.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motive outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61137/1097822199.py:11: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  result = grouped.with_columns([\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "# CPCNN\n",
    "pred = pl.read_parquet(f\"{output_dir}/cpcnn/mad_featselect/classifier_results/motive_binary_predictions.parquet\")\n",
    "class_balance = pred.select([\"Metadata_AggType\", \"Metadata_Label\", \"Metadata_Count_0\", \"Metadata_Count_1\"]).unique()\n",
    "\n",
    "grouped = pred.group_by(['Metadata_AggType', 'Metadata_Label']).agg([\n",
    "    pl.col('y_pred').alias('y_pred_list'),\n",
    "    pl.col('y_actual').alias('y_actual_list'),\n",
    "    pl.col('y_prob').alias('y_prob_list'),\n",
    "])\n",
    "\n",
    "result = grouped.with_columns([\n",
    "    pl.struct(['y_pred_list', 'y_actual_list', 'y_prob_list']).map_elements(\n",
    "        lambda s: compute_metrics(s['y_pred_list'], s['y_actual_list'], s['y_prob_list'])\n",
    "    ).alias('metrics')\n",
    "])\n",
    "\n",
    "result = result.with_columns([\n",
    "    pl.col('metrics').map_elements(lambda s: s[0]).alias('F1_Score'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('AUROC'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[2]).alias('PRAUC'),\n",
    "])\n",
    "\n",
    "result = result.drop(['y_pred_list', 'y_actual_list', 'y_prob_list', 'metrics'])\n",
    "cpcnn = result.join(class_balance, on=['Metadata_AggType', 'Metadata_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61137/517955348.py:11: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  result = grouped.with_columns([\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "## Dino\n",
    "pred = pl.read_parquet(f\"{output_dir}/dino/mad_featselect/classifier_results/motive_binary_predictions.parquet\")\n",
    "class_balance = pred.select([\"Metadata_AggType\", \"Metadata_Label\", \"Metadata_Count_0\", \"Metadata_Count_1\"]).unique()\n",
    "\n",
    "grouped = pred.group_by(['Metadata_AggType', 'Metadata_Label']).agg([\n",
    "    pl.col('y_pred').alias('y_pred_list'),\n",
    "    pl.col('y_actual').alias('y_actual_list'),\n",
    "    pl.col('y_prob').alias('y_prob_list'),\n",
    "])\n",
    "\n",
    "result = grouped.with_columns([\n",
    "    pl.struct(['y_pred_list', 'y_actual_list', 'y_prob_list']).map_elements(\n",
    "        lambda s: compute_metrics(s['y_pred_list'], s['y_actual_list'], s['y_prob_list'])\n",
    "    ).alias('metrics')\n",
    "])\n",
    "\n",
    "result = result.with_columns([\n",
    "    pl.col('metrics').map_elements(lambda s: s[0]).alias('F1_Score'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('AUROC'),\n",
    "    pl.col('metrics').map_elements(lambda s: s[1]).alias('PRAUC'),\n",
    "])\n",
    "\n",
    "result = result.drop(['y_pred_list', 'y_actual_list', 'y_prob_list', 'metrics'])\n",
    "dino = result.join(class_balance, on=['Metadata_AggType', 'Metadata_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CellProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "cpcnn = cpcnn.with_columns(pl.lit(\"cpcnn\").alias(\"Feat_type\"))\n",
    "dino = dino.with_columns(pl.lit(\"dino\").alias(\"Feat_type\"))\n",
    "#cellprofiler = cellprofiler.with_columns(pl.lit(\"cellprofiler\").alias(\"Feat_type\"))\n",
    "\n",
    "#all_results = pl.concat([cpcnn, dino, cellprofiler], how=\"vertical\")\n",
    "all_results = pl.concat([cpcnn, dino], how=\"vertical\")\n",
    "all_results.write_parquet(f\"{results_dir}/compiled_motive_metrics.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axiom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
